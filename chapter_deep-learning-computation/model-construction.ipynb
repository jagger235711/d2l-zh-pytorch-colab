{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dca9252",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 层和块\n",
    ":label:`sec_model_construction`\n",
    "\n",
    "之前首次介绍神经网络时，我们关注的是具有单一输出的线性模型。\n",
    "在这里，整个模型只有一个输出。\n",
    "注意，单个神经网络\n",
    "（1）接受一些输入；\n",
    "（2）生成相应的标量输出；\n",
    "（3）具有一组相关 *参数*（parameters），更新这些参数可以优化某目标函数。\n",
    "\n",
    "然后，当考虑具有多个输出的网络时，\n",
    "我们利用矢量化算法来描述整层神经元。\n",
    "像单个神经元一样，层（1）接受一组输入，\n",
    "（2）生成相应的输出，\n",
    "（3）由一组可调整参数描述。\n",
    "当我们使用softmax回归时，一个单层本身就是模型。\n",
    "然而，即使我们随后引入了多层感知机，我们仍然可以认为该模型保留了上面所说的基本架构。\n",
    "\n",
    "对于多层感知机而言，整个模型及其组成层都是这种架构。\n",
    "整个模型接受原始输入（特征），生成输出（预测），\n",
    "并包含一些参数（所有组成层的参数集合）。\n",
    "同样，每个单独的层接收输入（由前一层提供），\n",
    "生成输出（到下一层的输入），并且具有一组可调参数，\n",
    "这些参数根据从下一层反向传播的信号进行更新。\n",
    "\n",
    "事实证明，研究讨论“比单个层大”但“比整个模型小”的组件更有价值。\n",
    "例如，在计算机视觉中广泛流行的ResNet-152架构就有数百层，\n",
    "这些层是由*层组*（groups of layers）的重复模式组成。\n",
    "这个ResNet架构赢得了2015年ImageNet和COCO计算机视觉比赛\n",
    "的识别和检测任务 :cite:`He.Zhang.Ren.ea.2016`。\n",
    "目前ResNet架构仍然是许多视觉任务的首选架构。\n",
    "在其他的领域，如自然语言处理和语音，\n",
    "层组以各种重复模式排列的类似架构现在也是普遍存在。\n",
    "\n",
    "为了实现这些复杂的网络，我们引入了神经网络*块*的概念。\n",
    "*块*（block）可以描述单个层、由多个层组成的组件或整个模型本身。\n",
    "使用块进行抽象的一个好处是可以将一些块组合成更大的组件，\n",
    "这一过程通常是递归的，如 :numref:`fig_blocks`所示。\n",
    "通过定义代码来按需生成任意复杂度的块，\n",
    "我们可以通过简洁的代码实现复杂的神经网络。\n",
    "\n",
    "![多个层被组合成块，形成更大的模型](http://d2l.ai/_images/blocks.svg)\n",
    ":label:`fig_blocks`\n",
    "\n",
    "从编程的角度来看，块由*类*（class）表示。\n",
    "它的任何子类都必须定义一个将其输入转换为输出的前向传播函数，\n",
    "并且必须存储任何必需的参数。\n",
    "注意，有些块不需要任何参数。\n",
    "最后，为了计算梯度，块必须具有反向传播函数。\n",
    "在定义我们自己的块时，由于自动微分（在 :numref:`sec_autograd` 中引入）\n",
    "提供了一些后端实现，我们只需要考虑前向传播函数和必需的参数。\n",
    "\n",
    "在构造自定义块之前，(**我们先回顾一下多层感知机**)\n",
    "（ :numref:`sec_mlp_concise` ）的代码。\n",
    "下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层，\n",
    "然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9895e279",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:00.244437Z",
     "iopub.status.busy": "2023-08-18T06:57:00.243813Z",
     "iopub.status.idle": "2023-08-18T06:57:01.320999Z",
     "shell.execute_reply": "2023-08-18T06:57:01.320186Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0343,  0.0264,  0.2505, -0.0243,  0.0945,  0.0012, -0.0141,  0.0666,\n",
       "         -0.0547, -0.0667],\n",
       "        [ 0.0772, -0.0274,  0.2638, -0.0191,  0.0394, -0.0324,  0.0102,  0.0707,\n",
       "         -0.1481, -0.1031]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be949c0e",
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "在这个例子中，我们通过实例化`nn.Sequential`来构建我们的模型，\n",
    "层的执行顺序是作为参数传递的。\n",
    "简而言之，(**`nn.Sequential`定义了一种特殊的`Module`**)，\n",
    "即在PyTorch中表示一个块的类，\n",
    "它维护了一个由`Module`组成的有序列表。\n",
    "注意，两个全连接层都是`Linear`类的实例，\n",
    "`Linear`类本身就是`Module`的子类。\n",
    "另外，到目前为止，我们一直在通过`net(X)`调用我们的模型来获得模型的输出。\n",
    "这实际上是`net.__call__(X)`的简写。\n",
    "这个前向传播函数非常简单：\n",
    "它将列表中的每个块连接在一起，将每个块的输出作为下一个块的输入。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce5ce8",
   "metadata": {
    "origin_pos": 9
   },
   "source": [
    "## [**自定义块**]\n",
    "\n",
    "要想直观地了解块是如何工作的，最简单的方法就是自己实现一个。\n",
    "在实现我们自定义块之前，我们简要总结一下每个块必须提供的基本功能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea84f7",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "1. 将输入数据作为其前向传播函数的参数。\n",
    "1. 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收一个20维的输入，但是返回一个维度为256的输出。\n",
    "1. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "1. 存储和访问前向传播计算所需的参数。\n",
    "1. 根据需要初始化模型参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572894df",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "在下面的代码片段中，我们从零开始编写一个块。\n",
    "它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。\n",
    "注意，下面的`MLP`类继承了表示块的类。\n",
    "我们的实现只需要提供我们自己的构造函数（Python中的`__init__`函数）和前向传播函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876df867",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.325541Z",
     "iopub.status.busy": "2023-08-18T06:57:01.324828Z",
     "iopub.status.idle": "2023-08-18T06:57:01.330411Z",
     "shell.execute_reply": "2023-08-18T06:57:01.329591Z"
    },
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8327a09c",
   "metadata": {
    "origin_pos": 17
   },
   "source": [
    "我们首先看一下前向传播函数，它以`X`作为输入，\n",
    "计算带有激活函数的隐藏表示，并输出其未规范化的输出值。\n",
    "在这个`MLP`实现中，两个层都是实例变量。\n",
    "要了解这为什么是合理的，可以想象实例化两个多层感知机（`net1`和`net2`），\n",
    "并根据不同的数据对它们进行训练。\n",
    "当然，我们希望它们学到两种不同的模型。\n",
    "\n",
    "接着我们[**实例化多层感知机的层，然后在每次调用前向传播函数时调用这些层**]。\n",
    "注意一些关键细节：\n",
    "首先，我们定制的`__init__`函数通过`super().__init__()`\n",
    "调用父类的`__init__`函数，\n",
    "省去了重复编写模版代码的痛苦。\n",
    "然后，我们实例化两个全连接层，\n",
    "分别为`self.hidden`和`self.out`。\n",
    "注意，除非我们实现一个新的运算符，\n",
    "否则我们不必担心反向传播函数或参数初始化，\n",
    "系统将自动生成这些。\n",
    "\n",
    "我们来试一下这个函数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a34ec3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.334346Z",
     "iopub.status.busy": "2023-08-18T06:57:01.333603Z",
     "iopub.status.idle": "2023-08-18T06:57:01.340473Z",
     "shell.execute_reply": "2023-08-18T06:57:01.339676Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0669,  0.2202, -0.0912, -0.0064,  0.1474, -0.0577, -0.3006,  0.1256,\n",
       "         -0.0280,  0.4040],\n",
       "        [ 0.0545,  0.2591, -0.0297,  0.1141,  0.1887,  0.0094, -0.2686,  0.0732,\n",
       "         -0.0135,  0.3865]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aaa7fc",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "块的一个主要优点是它的多功能性。\n",
    "我们可以子类化块以创建层（如全连接层的类）、\n",
    "整个模型（如上面的`MLP`类）或具有中等复杂度的各种组件。\n",
    "我们在接下来的章节中充分利用了这种多功能性，\n",
    "比如在处理卷积神经网络时。\n",
    "\n",
    "## [**顺序块**]\n",
    "\n",
    "现在我们可以更仔细地看看`Sequential`类是如何工作的，\n",
    "回想一下`Sequential`的设计是为了把其他模块串起来。\n",
    "为了构建我们自己的简化的`MySequential`，\n",
    "我们只需要定义两个关键函数：\n",
    "\n",
    "1. 一种将块逐个追加到列表中的函数；\n",
    "1. 一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”。\n",
    "\n",
    "下面的`MySequential`类提供了与默认`Sequential`类相同的功能。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd09709c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.344392Z",
     "iopub.status.busy": "2023-08-18T06:57:01.343695Z",
     "iopub.status.idle": "2023-08-18T06:57:01.349458Z",
     "shell.execute_reply": "2023-08-18T06:57:01.348481Z"
    },
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a44d091",
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "`__init__`函数将每个模块逐个添加到有序字典`_modules`中。\n",
    "读者可能会好奇为什么每个`Module`都有一个`_modules`属性？\n",
    "以及为什么我们使用它而不是自己定义一个Python列表？\n",
    "简而言之，`_modules`的主要优点是：\n",
    "在模块的参数初始化过程中，\n",
    "系统知道在`_modules`字典中查找需要初始化参数的子块。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0272bce5",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "当`MySequential`的前向传播函数被调用时，\n",
    "每个添加的块都按照它们被添加的顺序执行。\n",
    "现在可以使用我们的`MySequential`类重新实现多层感知机。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9672de9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.353302Z",
     "iopub.status.busy": "2023-08-18T06:57:01.352727Z",
     "iopub.status.idle": "2023-08-18T06:57:01.360268Z",
     "shell.execute_reply": "2023-08-18T06:57:01.359462Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2759e-01, -4.7003e-02,  4.2846e-01, -1.2546e-01,  1.5296e-01,\n",
       "          1.8972e-01,  9.7048e-02,  4.5479e-04, -3.7986e-02,  6.4842e-02],\n",
       "        [ 2.7825e-01, -9.7517e-02,  4.8541e-01, -2.4519e-01, -8.4580e-02,\n",
       "          2.8538e-01,  3.6861e-02,  2.9411e-02, -1.0612e-01,  1.2620e-01]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189aa472",
   "metadata": {
    "origin_pos": 33
   },
   "source": [
    "请注意，`MySequential`的用法与之前为`Sequential`类编写的代码相同\n",
    "（如 :numref:`sec_mlp_concise` 中所述）。\n",
    "\n",
    "## [**在前向传播函数中执行代码**]\n",
    "\n",
    "`Sequential`类使模型构造变得简单，\n",
    "允许我们组合新的架构，而不必定义自己的类。\n",
    "然而，并不是所有的架构都是简单的顺序架构。\n",
    "当需要更强的灵活性时，我们需要定义自己的块。\n",
    "例如，我们可能希望在前向传播函数中执行Python的控制流。\n",
    "此外，我们可能希望执行任意的数学运算，\n",
    "而不是简单地依赖预定义的神经网络层。\n",
    "\n",
    "到目前为止，\n",
    "我们网络中的所有操作都对网络的激活值及网络的参数起作用。\n",
    "然而，有时我们可能希望合并既不是上一层的结果也不是可更新参数的项，\n",
    "我们称之为*常数参数*（constant parameter）。\n",
    "例如，我们需要一个计算函数\n",
    "$f(\\mathbf{x},\\mathbf{w}) = c \\cdot \\mathbf{w}^\\top \\mathbf{x}$的层，\n",
    "其中$\\mathbf{x}$是输入，\n",
    "$\\mathbf{w}$是参数，\n",
    "$c$是某个在优化过程中没有更新的指定常量。\n",
    "因此我们实现了一个`FixedHiddenMLP`类，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad09596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.364000Z",
     "iopub.status.busy": "2023-08-18T06:57:01.363468Z",
     "iopub.status.idle": "2023-08-18T06:57:01.369665Z",
     "shell.execute_reply": "2023-08-18T06:57:01.368755Z"
    },
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06017344",
   "metadata": {
    "origin_pos": 38
   },
   "source": [
    "在这个`FixedHiddenMLP`模型中，我们实现了一个隐藏层，\n",
    "其权重（`self.rand_weight`）在实例化时被随机初始化，之后为常量。\n",
    "这个权重不是一个模型参数，因此它永远不会被反向传播更新。\n",
    "然后，神经网络将这个固定层的输出通过一个全连接层。\n",
    "\n",
    "注意，在返回输出之前，模型做了一些不寻常的事情：\n",
    "它运行了一个while循环，在$L_1$范数大于$1$的条件下，\n",
    "将输出向量除以$2$，直到它满足条件为止。\n",
    "最后，模型返回了`X`中所有项的和。\n",
    "注意，此操作可能不会常用于在任何实际任务中，\n",
    "我们只展示如何将任意代码集成到神经网络计算的流程中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ebc567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.373508Z",
     "iopub.status.busy": "2023-08-18T06:57:01.372789Z",
     "iopub.status.idle": "2023-08-18T06:57:01.380049Z",
     "shell.execute_reply": "2023-08-18T06:57:01.379025Z"
    },
    "origin_pos": 40,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1862, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b18eb2",
   "metadata": {
    "origin_pos": 41
   },
   "source": [
    "我们可以[**混合搭配各种组合块的方法**]。\n",
    "在下面的例子中，我们以一些想到的方法嵌套块。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca3b399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T06:57:01.384091Z",
     "iopub.status.busy": "2023-08-18T06:57:01.383236Z",
     "iopub.status.idle": "2023-08-18T06:57:01.394649Z",
     "shell.execute_reply": "2023-08-18T06:57:01.393535Z"
    },
    "origin_pos": 43,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2183, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12e280",
   "metadata": {
    "origin_pos": 46
   },
   "source": [
    "## 效率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26229d3",
   "metadata": {
    "origin_pos": 48,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "读者可能会开始担心操作效率的问题。\n",
    "毕竟，我们在一个高性能的深度学习库中进行了大量的字典查找、\n",
    "代码执行和许多其他的Python代码。\n",
    "Python的问题[全局解释器锁](https://wiki.python.org/moin/GlobalInterpreterLock)\n",
    "是众所周知的。\n",
    "在深度学习环境中，我们担心速度极快的GPU可能要等到CPU运行Python代码后才能运行另一个作业。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa617e6",
   "metadata": {
    "origin_pos": 51
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "* 块可以包含代码。\n",
    "* 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "* 层和块的顺序连接由`Sequential`块处理。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 如果将`MySequential`中存储块的方式更改为Python列表，会出现什么样的问题？\n",
    "1. 实现一个块，它以两个块为参数，例如`net1`和`net2`，并返回前向传播中两个网络的串联输出。这也被称为平行块。\n",
    "1. 假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc34bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-02 23:42:43.749811\n",
      "使用_modules构建的网络：\n",
      "nettensor([[ 0.2175, -0.0927, -0.1648, -0.2101, -0.4045,  0.0931, -0.0378,  0.1357,\n",
      "          0.3302, -0.0201]], grad_fn=<AddmmBackward0>)\n",
      "net_listtensor([[ 0.3731, -0.2809,  0.4660, -0.0204,  0.0359,  0.1167, -0.2796, -0.0167,\n",
      "         -0.2883,  0.0601]], grad_fn=<AddmmBackward0>)\n",
      "net MySequential(\n",
      "  (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=20, out_features=10, bias=True)\n",
      ") \n",
      " OrderedDict([('0.weight', tensor([[-3.7285e-02,  1.5642e-01,  8.3753e-02,  9.0733e-02, -2.7807e-02,\n",
      "          5.8402e-02,  2.7955e-01, -2.2686e-01, -1.1135e-01,  1.9354e-01],\n",
      "        [-1.2339e-01,  1.8890e-01,  7.0568e-02,  2.2030e-01,  1.8596e-01,\n",
      "         -1.2804e-01, -2.4369e-02, -2.4155e-01, -1.4666e-01, -1.7872e-01],\n",
      "        [-1.6214e-01, -2.4237e-01, -2.9907e-01,  1.6388e-01, -3.7799e-02,\n",
      "         -2.5017e-01, -2.8916e-02, -1.0010e-01, -1.0258e-01, -2.2336e-01],\n",
      "        [ 2.7936e-01, -6.4795e-02,  1.1188e-01, -2.9003e-01,  1.5974e-01,\n",
      "          1.4862e-01,  1.4080e-02, -1.1596e-01,  2.2738e-01,  2.6794e-01],\n",
      "        [-7.7302e-02,  1.3354e-02, -7.1812e-02, -3.0665e-01,  1.5217e-02,\n",
      "          3.2907e-02, -2.5068e-02, -2.4739e-01,  4.3958e-02, -1.4569e-01],\n",
      "        [ 7.0358e-02,  5.8689e-02,  5.7401e-02, -1.1940e-01,  2.8469e-01,\n",
      "         -2.1731e-01, -1.5484e-01, -2.7332e-01,  1.5547e-01,  2.9449e-01],\n",
      "        [-1.1643e-01, -2.3409e-01, -1.5540e-01, -2.8559e-01,  2.5000e-01,\n",
      "         -1.2342e-01, -1.8994e-01, -1.7513e-01, -1.5919e-01, -2.4768e-01],\n",
      "        [-2.8999e-01, -1.2664e-01, -2.8741e-01,  7.7901e-02,  1.0687e-02,\n",
      "          3.0786e-03,  1.3345e-01, -2.2341e-01, -2.0529e-01, -3.1546e-01],\n",
      "        [ 1.0963e-01, -9.5346e-02, -2.2416e-01,  1.7083e-01, -2.5885e-01,\n",
      "         -2.2946e-01, -4.5786e-02,  1.1444e-01, -3.0351e-01, -2.8719e-01],\n",
      "        [ 2.6661e-01,  6.7518e-03, -1.6057e-01,  2.7040e-02, -1.2185e-01,\n",
      "         -4.9675e-02, -1.3001e-01, -7.6074e-02, -1.7313e-01, -9.3437e-02],\n",
      "        [-9.3182e-02, -2.0297e-01,  1.6834e-01, -6.4420e-02, -2.7390e-01,\n",
      "          9.1888e-02, -5.4584e-02, -1.6162e-01, -2.7135e-03,  1.5220e-01],\n",
      "        [ 2.4345e-01, -1.4595e-01, -2.9947e-01,  7.5321e-02,  2.6257e-01,\n",
      "         -5.0210e-02, -2.5109e-01,  1.3726e-01,  2.1337e-01,  1.3766e-01],\n",
      "        [-1.9976e-01,  3.0066e-02, -1.7753e-01, -2.0679e-01,  3.1400e-01,\n",
      "         -5.7196e-02,  1.4479e-01,  2.1412e-01, -1.6380e-01, -2.0136e-02],\n",
      "        [-2.8006e-01,  2.9367e-01, -6.8114e-02, -1.2056e-01, -6.8259e-02,\n",
      "         -6.1154e-02, -2.3423e-01,  1.4267e-01, -6.4790e-03, -1.0119e-01],\n",
      "        [ 4.9123e-02,  2.2045e-01,  1.6014e-01, -9.2073e-02, -1.2412e-01,\n",
      "          2.9695e-01,  7.9209e-02,  2.1727e-01,  3.0834e-01, -7.1729e-02],\n",
      "        [-1.3195e-01, -2.4030e-02, -2.5868e-02, -2.4186e-01,  1.9663e-01,\n",
      "          1.2719e-01,  2.4877e-01,  7.8867e-02,  1.4046e-01,  1.0513e-01],\n",
      "        [ 2.9055e-01, -2.5701e-01,  2.6330e-01, -7.6709e-02,  6.8160e-03,\n",
      "         -1.3641e-01, -2.0341e-01, -6.3270e-02, -2.4390e-01, -1.2836e-01],\n",
      "        [ 6.0712e-02,  4.0366e-02,  1.3008e-01,  3.3629e-02, -2.0218e-01,\n",
      "          8.0291e-02,  2.0392e-01, -2.8493e-01,  1.0114e-01,  2.7666e-04],\n",
      "        [ 1.2736e-01, -1.6273e-01,  9.4803e-02, -3.1146e-01, -2.7706e-01,\n",
      "          1.5459e-01,  2.2491e-01, -1.8900e-01, -2.8983e-01,  1.3091e-01],\n",
      "        [-1.1243e-01,  1.2375e-02,  2.5156e-01, -8.9580e-02, -2.0177e-01,\n",
      "          2.4844e-01, -2.9741e-01,  1.5436e-02,  4.2400e-02, -9.5153e-02]])), ('0.bias', tensor([ 0.2148, -0.2466,  0.0799,  0.2004,  0.0166,  0.2626, -0.1174, -0.1284,\n",
      "        -0.2976, -0.1195,  0.0777,  0.0117, -0.0090, -0.1633, -0.0781, -0.0545,\n",
      "         0.2766, -0.0374,  0.2103,  0.2434])), ('2.weight', tensor([[-0.1003, -0.1674,  0.1991, -0.0559, -0.2139,  0.1089,  0.0269, -0.1057,\n",
      "          0.1348,  0.2029, -0.0681, -0.0529,  0.1101,  0.1041,  0.1093,  0.1726,\n",
      "         -0.1101,  0.1217, -0.1642,  0.0666],\n",
      "        [-0.1861, -0.1343,  0.1180,  0.0432,  0.0412,  0.0701,  0.0620,  0.0583,\n",
      "          0.0711,  0.1229,  0.2096, -0.1905,  0.1704,  0.0766, -0.1952, -0.1162,\n",
      "          0.0273, -0.1528,  0.1147,  0.0381],\n",
      "        [ 0.0970,  0.1933, -0.0608,  0.0009,  0.0567,  0.1371, -0.1629,  0.2216,\n",
      "          0.0477, -0.1873,  0.1649, -0.1599,  0.0031,  0.1004, -0.1956, -0.0983,\n",
      "          0.0431,  0.1082, -0.1435, -0.1679],\n",
      "        [-0.1660, -0.0445,  0.0852, -0.1051, -0.2072, -0.0304, -0.1002,  0.0474,\n",
      "         -0.0861,  0.1413, -0.1015,  0.1448, -0.0231,  0.0398, -0.2046, -0.1505,\n",
      "         -0.1619,  0.1345, -0.1169,  0.1039],\n",
      "        [ 0.0135,  0.0077,  0.1805, -0.1927,  0.1110, -0.1881, -0.0864, -0.1860,\n",
      "         -0.1922,  0.1795, -0.1985,  0.0190,  0.0688,  0.0158, -0.2107, -0.1122,\n",
      "          0.1632,  0.1467, -0.1153,  0.1271],\n",
      "        [-0.0538,  0.1319, -0.2079,  0.0857, -0.0440,  0.0102, -0.0861, -0.0059,\n",
      "         -0.1477,  0.0489,  0.0444,  0.0194,  0.1359,  0.0583,  0.1464,  0.1560,\n",
      "          0.0904, -0.1140, -0.0709, -0.1863],\n",
      "        [-0.0774, -0.0084, -0.0483,  0.0442, -0.2019,  0.0291, -0.0867, -0.2230,\n",
      "         -0.2208, -0.1044,  0.1122,  0.0572, -0.2174, -0.0093, -0.1736, -0.0809,\n",
      "          0.0845, -0.0588,  0.1197, -0.1078],\n",
      "        [-0.1961,  0.1190,  0.1024,  0.1195,  0.1873, -0.1848, -0.0924, -0.0879,\n",
      "         -0.0661,  0.2046, -0.0939,  0.0629, -0.0931, -0.1898, -0.0208,  0.1910,\n",
      "          0.1311,  0.1111,  0.0723, -0.1486],\n",
      "        [ 0.1300, -0.1900,  0.1538, -0.0358, -0.0284,  0.2013,  0.2233,  0.0958,\n",
      "         -0.0644,  0.0913, -0.0507,  0.1402, -0.0790,  0.1196,  0.0652,  0.0112,\n",
      "         -0.1501,  0.0112,  0.1551, -0.0717],\n",
      "        [-0.1006,  0.1796,  0.2123,  0.0597, -0.1087, -0.0817, -0.1029, -0.1755,\n",
      "          0.1019, -0.0736,  0.0831, -0.0185, -0.0030,  0.1789,  0.0087,  0.1481,\n",
      "         -0.0775, -0.2122, -0.1243,  0.1129]])), ('2.bias', tensor([ 0.1571,  0.0497, -0.1346,  0.0176, -0.1896,  0.0317,  0.0672,  0.1884,\n",
      "         0.2131, -0.0081]))])\n",
      "net_list MySequential_list() \n",
      " OrderedDict()\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# print一个时间戳\n",
    "import datetime\n",
    "\n",
    "print(datetime.datetime.now())  # 打印当前日期和时间\n",
    "\n",
    "\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class MySequential_list(nn.Module):\n",
    "    # 使用list\n",
    "    def __init__(self, *args):\n",
    "        # super(MySequential_list, self).__init__()\n",
    "        super().__init__()\n",
    "        self.sequential = []\n",
    "        for module in args:\n",
    "            self.sequential.append(module)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for module in self.sequential:\n",
    "            X = module(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "X = torch.rand(1, 10)\n",
    "net = MySequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 10))\n",
    "net_list = MySequential_list(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 10))\n",
    "\n",
    "# 分割线\n",
    "print(\"使用_modules构建的网络：\")\n",
    "# 结果一样\n",
    "print(\"net\" + str(net(X)))\n",
    "print(\"net_list\" + str(net_list(X)))\n",
    "# 使用_modules方便打印net的网络结构和参数，而list则无法做到\n",
    "print(\"net\", net, \"\\n\", net.state_dict())\n",
    "print(\"net_list\", net_list, \"\\n\", net_list.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "756e0741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 36])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class Parallel(nn.Module):\n",
    "    def __init__(self, net1, net2):\n",
    "        super().__init__()\n",
    "        self.net1 = net1  # 第一个子网络\n",
    "        self.net2 = net2  # 第二个子网络\n",
    "\n",
    "    def forward(self, X):\n",
    "        x1 = self.net1(X)  # 第一个子网络的输出\n",
    "        x2 = self.net2(X)  # 第二个子网络的输出\n",
    "        return torch.cat((x1, x2), dim=1)  # 在通道维度上合并输出结果\n",
    "\n",
    "\n",
    "X = torch.rand(2, 10)  # 输入数据\n",
    "net = Parallel(\n",
    "    nn.Sequential(nn.Linear(10, 12), nn.ReLU()),\n",
    "    nn.Sequential(nn.Linear(10, 24), nn.ReLU()),\n",
    ")  # 实例化并行网络\n",
    "output = net(X)\n",
    "output.size()  # 输出结果的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe27d64d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.0.weight',\n",
       "              tensor([[-0.2500,  0.2120, -0.0722, -0.2493,  0.2813,  0.2781,  0.2507, -0.0161,\n",
       "                       -0.2431,  0.1445],\n",
       "                      [ 0.1241, -0.0080,  0.0309, -0.2603, -0.1185,  0.0293, -0.2308, -0.0344,\n",
       "                        0.2725,  0.0583],\n",
       "                      [ 0.2467, -0.1879, -0.0973,  0.1998, -0.1370,  0.2092,  0.2551, -0.1347,\n",
       "                        0.1264,  0.0628],\n",
       "                      [ 0.2235,  0.1986, -0.2631,  0.2237, -0.1921, -0.0627, -0.2036,  0.2576,\n",
       "                       -0.0194, -0.2437],\n",
       "                      [-0.0669, -0.2851, -0.0737, -0.2554, -0.1279, -0.3111, -0.0438,  0.1669,\n",
       "                       -0.3143,  0.0372]])),\n",
       "             ('0.0.bias',\n",
       "              tensor([-0.0727,  0.1170, -0.0518, -0.2234,  0.1148])),\n",
       "             ('0.2.weight',\n",
       "              tensor([[-0.3332, -0.3731, -0.3879,  0.3411,  0.0995],\n",
       "                      [-0.3597, -0.3364,  0.2723,  0.3077,  0.3312],\n",
       "                      [-0.0855,  0.1665, -0.1469,  0.0455,  0.3012],\n",
       "                      [ 0.1152,  0.4346, -0.1673,  0.1440,  0.2854],\n",
       "                      [-0.0793,  0.0948, -0.1382, -0.1350,  0.1495],\n",
       "                      [-0.3319,  0.3526,  0.4387,  0.4237, -0.2479],\n",
       "                      [-0.4350, -0.1083, -0.2466,  0.0439,  0.0268],\n",
       "                      [ 0.3551, -0.1771, -0.2567,  0.1247, -0.1811],\n",
       "                      [-0.1711,  0.0460, -0.0483, -0.1529,  0.1033],\n",
       "                      [-0.1088, -0.1536,  0.0067,  0.1790, -0.0219]])),\n",
       "             ('0.2.bias',\n",
       "              tensor([ 0.3783, -0.1183, -0.0777,  0.3227, -0.4341,  0.1420,  0.0024, -0.1111,\n",
       "                       0.1283,  0.3394])),\n",
       "             ('1.0.weight',\n",
       "              tensor([[-0.2354, -0.2994, -0.2304, -0.2927, -0.1659, -0.2586,  0.0271, -0.1667,\n",
       "                       -0.2092, -0.2132],\n",
       "                      [-0.0049, -0.1124,  0.0588, -0.0709, -0.1834,  0.2876,  0.0481,  0.1359,\n",
       "                       -0.1637,  0.2846],\n",
       "                      [ 0.1132, -0.1137, -0.2586,  0.0539, -0.1718, -0.0860, -0.1431,  0.1473,\n",
       "                        0.1783,  0.2786],\n",
       "                      [-0.0307,  0.2729, -0.1979, -0.2896,  0.2687, -0.2243, -0.1613,  0.1688,\n",
       "                        0.2157,  0.0005],\n",
       "                      [-0.1707, -0.1337,  0.1051, -0.0238,  0.2597,  0.0867, -0.1397,  0.0441,\n",
       "                       -0.2429,  0.2048]])),\n",
       "             ('1.0.bias',\n",
       "              tensor([ 0.0149, -0.1920,  0.0476,  0.2922,  0.2692])),\n",
       "             ('1.2.weight',\n",
       "              tensor([[ 0.4034, -0.1093, -0.1427,  0.4041,  0.1818],\n",
       "                      [-0.1773, -0.1657,  0.2495, -0.3014, -0.0553],\n",
       "                      [-0.2205,  0.2120, -0.4315, -0.0072,  0.1003],\n",
       "                      [ 0.3480,  0.1676,  0.1717,  0.2780, -0.3353],\n",
       "                      [ 0.0578,  0.2324,  0.1244,  0.2086, -0.0233],\n",
       "                      [-0.3779,  0.3969, -0.0514, -0.1262,  0.4202],\n",
       "                      [ 0.4377,  0.1943, -0.3500,  0.0356,  0.0244],\n",
       "                      [ 0.2487,  0.0876, -0.2638, -0.2347, -0.2200],\n",
       "                      [-0.2435,  0.4206, -0.4015, -0.1427,  0.3525],\n",
       "                      [-0.1991, -0.1662, -0.1250, -0.2007,  0.0846]])),\n",
       "             ('1.2.bias',\n",
       "              tensor([-0.2965,  0.0495, -0.1867,  0.2246, -0.0237, -0.1980,  0.3715, -0.2883,\n",
       "                       0.0056,  0.2070])),\n",
       "             ('2.0.weight',\n",
       "              tensor([[ 0.2457, -0.1780, -0.0507,  0.0929,  0.0770, -0.1698,  0.0774, -0.1045,\n",
       "                        0.3084,  0.0363],\n",
       "                      [-0.2208, -0.0813,  0.2832,  0.1434,  0.3108, -0.0222, -0.1752,  0.2095,\n",
       "                       -0.2596, -0.2500],\n",
       "                      [ 0.1718, -0.0333, -0.0269, -0.0869, -0.0026,  0.2411, -0.1894, -0.2920,\n",
       "                       -0.1721, -0.2710],\n",
       "                      [-0.0990, -0.1448, -0.3157,  0.2799, -0.2977, -0.2492, -0.2952,  0.0307,\n",
       "                       -0.2409,  0.3076],\n",
       "                      [-0.2577, -0.2454, -0.1084,  0.2802,  0.0234,  0.0119,  0.0658, -0.1276,\n",
       "                        0.2529, -0.1428]])),\n",
       "             ('2.0.bias',\n",
       "              tensor([ 0.2363, -0.1079, -0.1245, -0.1954, -0.2606])),\n",
       "             ('2.2.weight',\n",
       "              tensor([[-0.2504, -0.0053, -0.0199, -0.0999,  0.2831],\n",
       "                      [ 0.2200, -0.1359,  0.2012,  0.2074,  0.0363],\n",
       "                      [-0.4251,  0.2243,  0.0035,  0.2730,  0.2835],\n",
       "                      [-0.2895, -0.0007, -0.1199,  0.0641,  0.2662],\n",
       "                      [-0.1654,  0.1486, -0.4088, -0.2944, -0.0727],\n",
       "                      [ 0.0131,  0.3037,  0.2875, -0.1182, -0.0469],\n",
       "                      [-0.3061, -0.2719, -0.2512, -0.1564,  0.1998],\n",
       "                      [ 0.3713,  0.2869,  0.2515,  0.4195,  0.1277],\n",
       "                      [-0.1669, -0.2319, -0.0250,  0.3683,  0.1289],\n",
       "                      [-0.2341,  0.0339, -0.3384,  0.2564, -0.0066]])),\n",
       "             ('2.2.bias',\n",
       "              tensor([ 0.0119, -0.2620,  0.1875,  0.0718,  0.3493,  0.3888,  0.0365,  0.1368,\n",
       "                      -0.3563,  0.4303])),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.2588,  0.1048, -0.1909, -0.2314,  0.2974,  0.2160,  0.0036,  0.0228,\n",
       "                        0.1203,  0.0203],\n",
       "                      [ 0.1304, -0.1277,  0.3114, -0.3051,  0.1564, -0.2120,  0.0535, -0.1196,\n",
       "                        0.1381, -0.0230]])),\n",
       "             ('output.bias', tensor([0.2122, 0.0416]))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "def create_network(num_instances, input_size, hidden_size, output_size):\n",
    "    # # 创建一个线性层\n",
    "    # linear_layer = nn.Sequential(\n",
    "    #     nn.Linear(input_size, hidden_size),\n",
    "    #     nn.ReLU(),\n",
    "    #     nn.Linear(hidden_size, input_size),\n",
    "    # )\n",
    "\n",
    "    # # 创建多个实例并连接\n",
    "    # instances = [linear_layer for _ in range(num_instances)]\n",
    "\n",
    "    # 创建多个实例并连接，每次都新建一个nn.Sequential对象\n",
    "    instances = [\n",
    "        nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "        )\n",
    "        for _ in range(num_instances)\n",
    "    ]\n",
    "\n",
    "    network = nn.Sequential(*instances)\n",
    "\n",
    "    # 添加输出层\n",
    "    output_layer = nn.Linear(input_size, output_size)\n",
    "    network.add_module(\"output\", output_layer)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# 示例用法\n",
    "net = create_network(num_instances=3, input_size=10, hidden_size=5, output_size=2)\n",
    "net.state_dict()  # 打印网络参数字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29846c8",
   "metadata": {
    "origin_pos": 53,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1827)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
